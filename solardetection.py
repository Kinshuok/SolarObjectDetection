# -*- coding: utf-8 -*-
"""solardetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sr-9qIvtbsnLgIOFBaUJldsKNuyYLJHF
"""

# Commented out IPython magic to ensure Python compatibility.


# Change to YOLOv8 directory
# %cd /content/drive/MyDrive/YOLOv8

# Install YOLOv8
!pip install ultralytics

import os
import os

# List all files inside the extracted dataset folder
print(os.listdir("/content/drive/MyDrive/YOLOv8/dataset"))

yaml_content = """path: /content/drive/MyDrive/YOLOv8/dataset
train: images/train
val: images/val
test: images/test

nc: 3  # Number of classes
names: ["solar_panel_0", "solar_panel_1", "solar_panel_2"]
"""

# Save it as dataset.yaml inside the dataset folder
with open("/content/drive/MyDrive/YOLOv8//dataset.yaml", "w") as f:
    f.write(yaml_content)

print("\nâœ… dataset.yaml has been created successfully!")

from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8n.pt")  # Using YOLOv8 Nano model

# Train the model
model.train(
    data="/content/drive/MyDrive/YOLOv8/dataset.yaml",  # Correct path
    epochs=50,    # Number of training epochs
    imgsz=416,    # Image size
    batch=16,     # Adjust based on memory
    device="cuda"  # Use GPU
)

from ultralytics import YOLO
import os

# Load trained YOLO model
model = YOLO("/content/drive/MyDrive/YOLOv8/runs/detect/train2/weights/best.pt")

# Run inference on test images
results = model.predict("/content/drive/MyDrive/YOLOv8/dataset/images/test", save=True)

# Print output directory where predictions are saved
print("\nâœ… Predictions saved in: /content/runs/detect/predict/")

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install torchmetrics

import numpy as np
from shapely.geometry import box

def yolo_to_bbox(yolo_bbox, img_size=416):
    """
    Converts YOLO format bbox to (x_min, y_min, x_max, y_max)
    """
    x_c, y_c, w, h = yolo_bbox
    x_min = (x_c - w / 2) * img_size
    y_min = (y_c - h / 2) * img_size
    x_max = (x_c + w / 2) * img_size
    y_max = (y_c + h / 2) * img_size
    return x_min, y_min, x_max, y_max

def compute_iou(bbox1, bbox2):
    """
    Computes IoU between two bounding boxes.
    """
    box1 = box(*bbox1)
    box2 = box(*bbox2)

    # Compute intersection and union
    intersection = box1.intersection(box2).area
    union = box1.union(box2).area

    return intersection / union if union > 0 else 0

# Example test
iou_score = compute_iou([0, 50, 150, 150], [100, 100, 200, 200])
print(f"IoU Score: {iou_score:.4f}")

import torch
from torchmetrics.detection.mean_ap import MeanAveragePrecision
import os

# Initialize Mean Average Precision metric
map_metric = MeanAveragePrecision(iou_type="bbox")

# Convert YOLO results into the required format
predictions = []
ground_truths = []

# Create dictionaries to store ground truths and predictions by image name
gt_dict = {}
pred_dict = {}

# Extract ground truths from YOLO format
label_dir = "/content/drive/MyDrive/YOLOv8/dataset/labels/test"
for file in os.listdir(label_dir):
    if file.endswith(".txt"):
        img_name = file[:-4]  # Remove '.txt' to get image name
        gt_boxes = []
        gt_labels = []

        with open(os.path.join(label_dir, file), "r") as f:
            for line in f.readlines():
                parts = line.strip().split()
                cls, x_center, y_center, width, height = map(float, parts)
                x_min = (x_center - width / 2) * 416
                y_min = (y_center - height / 2) * 416
                x_max = (x_center + width / 2) * 416
                y_max = (y_center + height / 2) * 416
                gt_boxes.append([x_min, y_min, x_max, y_max])
                gt_labels.append(int(cls))

        gt_dict[img_name] = {
            "boxes": torch.tensor(gt_boxes),
            "labels": torch.tensor(gt_labels),
        }

# Extract predictions and align with ground truths
for r in results:
    img_name = os.path.basename(r.path)[:-4]  # Extract image name without extension

    pred_boxes = r.boxes.xyxy.tolist()  # Convert tensor to list
    pred_scores = r.boxes.conf.tolist()
    pred_labels = r.boxes.cls.int().tolist()

    # Store prediction data in pred_dict
    pred_dict[img_name] = {
        "boxes": torch.tensor(pred_boxes),
        "scores": torch.tensor(pred_scores),
        "labels": torch.tensor(pred_labels)
    }

    # Get ground truth for this image from gt_dict
    if img_name in gt_dict:
        ground_truths.append(gt_dict[img_name])
        predictions.append(pred_dict[img_name]) # Use data from pred_dict
    else:
        print(f"Warning: Ground truth not found for image: {img_name}")

# Compute mAP50
map_metric.update(predictions, ground_truths)
map_results = map_metric.compute()

print(f"\nâœ… mAP50: {map_results['map_50']:.4f}")



# Store IoU scores
iou_scores = {}

for img_name in gt_dict.keys():
    if img_name in pred_dict:
        gt_boxes = gt_dict[img_name]["boxes"].tolist()  # Convert tensor to list if needed
        pred_boxes = pred_dict[img_name]["boxes"].tolist()  # Convert tensor to list

        img_ious = []

        # Compute IoU for each ground truth box
        for gt_box in gt_boxes:
            max_iou = 0  # Initialize max IoU per GT box
            for pred_box in pred_boxes:
                if isinstance(gt_box, list) and isinstance(pred_box, list):
                    iou = compute_iou(gt_box, pred_box)  # Compute IoU
                    max_iou = max(max_iou, iou)  # Take the highest IoU match
                else:
                    print(f"Skipping invalid box format: {gt_box} or {pred_box}")

            img_ious.append(max_iou)

        iou_scores[img_name] = img_ious  # Store IoUs for this image

# Display results
for img, ious in iou_scores.items():
    print(f"\nâœ… {img}: IoU Scores â†’ {ious}")

# Flatten all IoUs
all_ious = [iou for ious in iou_scores.values() for iou in ious]

# Compute mean IoU
mean_iou = np.mean(all_ious) if len(all_ious) > 0 else 0

print(f"\nâœ… Mean IoU Across Test Set: {mean_iou:.4f}")

import numpy as np
import pandas as pd
import supervision as sv

# Define class names
class_names = ["solar_panel_0", "solar_panel_1", "solar_panel_2"]

# Define IoU and Confidence Thresholds
iou_thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
conf_thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]

# Store results in a table
results_table = []

for iou_thr in iou_thresholds:
    for conf_thr in conf_thresholds:
        filtered_predictions = []

        for img_name in gt_dict.keys():
            if img_name in pred_dict:
                pred_boxes = pred_dict[img_name]["boxes"].cpu().numpy()  # Convert to NumPy
                pred_scores = pred_dict[img_name]["scores"].cpu().numpy()  # Convert to NumPy
                pred_labels = pred_dict[img_name]["labels"].cpu().numpy()  # Convert to NumPy

                mask = pred_scores >= conf_thr  # Confidence filter

                # Ensure correct shape for sv.Detections
                filtered_boxes = pred_boxes[mask] if pred_boxes[mask].shape[0] > 0 else np.zeros((0, 4))
                filtered_scores = pred_scores[mask] if pred_scores[mask].shape[0] > 0 else np.array([])
                filtered_labels = pred_labels[mask] if pred_labels[mask].shape[0] > 0 else np.array([])

                filtered_predictions.append(
                    sv.Detections(
                        xyxy=filtered_boxes,
                        confidence=filtered_scores,
                        class_id=filtered_labels
                    )
                )
            else:
                filtered_predictions.append(sv.Detections.empty())  # Empty case

        # Compute confusion matrix
        conf_matrix = sv.ConfusionMatrix.from_detections(
            predictions=filtered_predictions,
            targets=[
                sv.Detections(
                    xyxy=gt_dict[img_name]["boxes"].cpu().numpy(),  # Convert GT to NumPy
                    class_id=gt_dict[img_name]["labels"].cpu().numpy()
                ) for img_name in gt_dict.keys()
            ],
            classes=class_names
        )

        # Convert confusion matrix to NumPy array
        matrix = conf_matrix.matrix.astype(np.int32)

        # Compute TP, FP, FN
        TP = np.diag(matrix).sum()  # Sum of diagonal elements (Correct Predictions)
        FP = matrix.sum(axis=0) - np.diag(matrix)  # Sum of each column minus TP (False Positives)
        FN = matrix.sum(axis=1) - np.diag(matrix)  # Sum of each row minus TP (False Negatives)

        # Compute Precision, Recall, and F1-score
        precision = TP / (TP + FP.sum()) if (TP + FP.sum()) > 0 else 0
        recall = TP / (TP + FN.sum()) if (TP + FN.sum()) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # Compute Accuracy
        correct_preds = TP
        total_preds = matrix.sum()
        accuracy = correct_preds / total_preds if total_preds > 0 else 0

        # Store results
        results_table.append({
            "IoU Threshold": iou_thr,
            "Confidence Threshold": conf_thr,
            "Accuracy": f"{accuracy:.2f}",
            "Precision": f"{precision:.2f}",
            "Recall": f"{recall:.2f}",
            "F1-Score": f"{f1_score:.2f}"
        })

# Convert results to DataFrame for display
df_results = pd.DataFrame(results_table)

# Display the table
print("\nâœ… Evaluation Metrics Table:\n")
print(df_results.to_markdown())

import numpy as np
from sklearn.metrics import precision_recall_curve, auc

def compute_ap_voc_11point(precisions, recalls):
    """
    Computes Average Precision (AP) using Pascal VOC 11-point interpolation method.
    """
    ap = 0.0
    recall_levels = np.linspace(0, 1, 11)  # [0, 0.1, ..., 1]
    for recall_level in recall_levels:
        p = np.max(precisions[recalls >= recall_level]) if np.any(recalls >= recall_level) else 0
        ap += p / 11.0
    return ap

def compute_ap_coco_101point(precisions, recalls):
    """
    Computes Average Precision (AP) using COCO 101-point interpolation method.
    """
    recall_levels = np.linspace(0, 1, 101)  # [0, 0.01, ..., 1]
    ap = np.mean([np.max(precisions[recalls >= r]) if np.any(recalls >= r) else 0 for r in recall_levels])
    return ap

def compute_ap_auc(precisions, recalls):
    """
    Computes AP using the Area Under Precision-Recall Curve method.
    """
    return auc(recalls, precisions)

# Store results
ap_results = []

for img_name in gt_dict.keys():
    if img_name in pred_dict:
        pred_scores = pred_dict[img_name]["scores"].tolist()
        gt_labels = gt_dict[img_name]["labels"].tolist()

        # Compute Precision-Recall Curve
        precisions, recalls, _ = precision_recall_curve(gt_labels, pred_scores)

        # Compute AP using different methods
        ap_voc = compute_ap_voc_11point(precisions, recalls)
        ap_coco = compute_ap_coco_101point(precisions, recalls)
        ap_auc = compute_ap_auc(precisions, recalls)

        ap_results.append([img_name, ap_voc, ap_coco, ap_auc])

# Convert results to DataFrame
df_ap = pd.DataFrame(ap_results, columns=["Image", "AP (VOC 11-point)", "AP (COCO 101-point)", "AP (AUC)"])

# Display the table
tools.display_dataframe_to_user(name="Average Precision Table", dataframe=df_ap)



# Find images that exist in both gt_dict and pred_dict
common_images = list(set(gt_dict.keys()) & set(pred_dict.keys()))
missing_in_gt = list(set(pred_dict.keys()) - set(gt_dict.keys()))
missing_in_pred = list(set(gt_dict.keys()) - set(pred_dict.keys()))

print(f"âœ… Common Images: {len(common_images)}")
print(f"âŒ Images in Predictions but missing in GT: {len(missing_in_gt)} â†’ {missing_in_gt[:5]}")
print(f"âŒ Images in GT but missing in Predictions: {len(missing_in_pred)} â†’ {missing_in_pred[:5]}")

mismatched_images = []

for img_name in gt_dict.keys():
    gt_labels_len = len(gt_dict[img_name]["labels"])
    pred_labels_len = len(pred_dict[img_name]["labels"])

    if gt_labels_len != pred_labels_len:
        mismatched_images.append((img_name, gt_labels_len, pred_labels_len))

if mismatched_images:
    print("\nðŸš¨ Mismatched Images Found:")
    for img, gt_len, pred_len in mismatched_images[:]:  # Show first 5 mismatches
        print(f"ðŸ”¹ {img}: GT Labels = {gt_len}, Pred Labels = {pred_len}")
        print(len(mismatched_images))
else:
    print("\nâœ… No mismatched images found. GT and Predictions are properly aligned.")

import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_curve, auc

# Function to compute AP using Pascal VOC 11-point interpolation method
def compute_ap_voc_11point(precisions, recalls):
    recall_levels = np.linspace(0, 1, 11)  # [0, 0.1, ..., 1]
    ap = np.mean([np.max(precisions[recalls >= r]) if np.any(recalls >= r) else 0 for r in recall_levels])
    return ap

# Function to compute AP using COCO 101-point interpolation method
def compute_ap_coco_101point(precisions, recalls):
    recall_levels = np.linspace(0, 1, 101)  # [0, 0.01, ..., 1]
    ap = np.mean([np.max(precisions[recalls >= r]) if np.any(recalls >= r) else 0 for r in recall_levels])
    return ap

# Function to compute AP using Area Under Precision-Recall Curve (AUC)
def compute_ap_auc(precisions, recalls):
    return auc(recalls, precisions)

# Store results
ap_results = []

# Unique classes in the dataset
unique_classes = [0, 1, 2]  # Modify this based on your dataset

for class_id in unique_classes:
    class_ap_results = []

    for img_name in gt_dict.keys():
        if img_name in pred_dict:
            # Extract GT labels and prediction scores
            gt_labels = np.array(gt_dict[img_name]["labels"])  # Convert to NumPy 1D array
            pred_scores = np.array(pred_dict[img_name]["scores"])  # Convert to NumPy 1D array
            pred_labels = np.array(pred_dict[img_name]["labels"])  # Convert to NumPy 1D array

            # Ensure labels and scores have the same length
            min_length = min(len(gt_labels), len(pred_scores))
            gt_labels = gt_labels[:min_length]
            pred_scores = pred_scores[:min_length]
            pred_labels = pred_labels[:min_length]

            # Convert GT labels to binary: 1 for current class, 0 for others
            gt_binary = (gt_labels == class_id).astype(int)
            pred_binary = (pred_labels == class_id).astype(int)

            # Compute Precision-Recall Curve
            if np.sum(gt_binary) > 0:  # Only compute if GT labels exist for this class
                precisions, recalls, _ = precision_recall_curve(gt_binary, pred_scores)

                # Compute AP using different methods
                ap_voc = compute_ap_voc_11point(precisions, recalls)
                ap_coco = compute_ap_coco_101point(precisions, recalls)
                ap_auc = compute_ap_auc(precisions, recalls)

                class_ap_results.append([img_name, ap_voc, ap_coco, ap_auc])

    # Convert results to DataFrame
    df_class_ap = pd.DataFrame(class_ap_results, columns=["Image", "AP (VOC 11-point)", "AP (COCO 101-point)", "AP (AUC)"])

    # Display the table for this class
    print(f"\nâœ… Average Precision (AP) Table for Class {class_id}:\n")
    print(df_class_ap.to_markdown())